{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/atishadhikari/fake-news-cleaning-word2vec-lstm-99-accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.5.0-cp38-cp38-win_amd64.whl (422.6 MB)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio~=1.34.0\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting wheel~=0.35\n",
      "  Using cached wheel-0.36.2-py2.py3-none-any.whl (35 kB)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\lilie\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow) (3.15.7)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.2)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.30.2-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (49.2.0.post20200714)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow) (2.24.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\lilie\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow) (2.10)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=51a9023835aa26c70017ffce5a3515c68a33a06743fffa9d7dccdd2ef0d1b41f\n",
      "  Stored in directory: c:\\users\\lilie\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=0feb2d5436751b8d43fac488abcbf395ab44824f673883f0241b2996f26295f9\n",
      "  Stored in directory: c:\\users\\lilie\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: absl-py, gast, grpcio, keras-preprocessing, wheel, termcolor, astunparse, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-data-server, tensorboard-plugin-wit, tensorboard, flatbuffers, wrapt, google-pasta, h5py, opt-einsum, keras-nightly, tensorflow-estimator, tensorflow\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Uninstalling wheel-0.34.2:\n",
      "      Successfully uninstalled wheel-0.34.2\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.11.2\n",
      "    Uninstalling wrapt-1.11.2:\n",
      "      Successfully uninstalled wrapt-1.11.2\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.10.0\n",
      "    Uninstalling h5py-2.10.0:\n",
      "      Successfully uninstalled h5py-2.10.0\n",
      "Successfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.2 flatbuffers-1.12 gast-0.4.0 google-auth-1.30.2 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 wheel-0.36.2 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\lilie\\OneDrive\\Bureau\\ironHack\\dataset\\fake_real_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_REAL</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You Can Smell Hillaryâ€™s Fear Daniel Greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_REAL                                                txt\n",
       "0           0  You Can Smell Hillaryâ€™s Fear Daniel Greenfield...\n",
       "1           0  Watch The Exact Moment Paul Ryan Committed Pol...\n",
       "2           1  Kerry to go to Paris in gesture of sympathy U....\n",
       "3           0  Bernie supporters on Twitter erupt in anger ag...\n",
       "4           1  The Battle of New York: Why This Primary Matte..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['txt'] = df['title'] + \" \" + df['text']\n",
    "df = df.drop(columns = ['title', 'text','subject','date'])\n",
    "df = df.loc[~df['txt'].isna()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"label_REAL\"].values\n",
    "#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\n",
    "X = []\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "for par in df[\"txt\"].values:\n",
    "    tmp = []\n",
    "    sentences = nltk.sent_tokenize(par)\n",
    "    for sent in sentences:\n",
    "        sent = sent.lower()\n",
    "        tokens = tokenizer.tokenize(sent)\n",
    "        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n",
    "        tmp.extend(filtered_words)\n",
    "    X.append(tmp)\n",
    "\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
    "w2v_model = gensim.models.Word2Vec(sentences=X, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "w = models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142830"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab size\n",
    "len(w2v_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now represented each of 142830 words by a 100dim vector (default value in Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tehran', 0.8926344513893127),\n",
       " ('iranian', 0.7594042420387268),\n",
       " ('nuclear', 0.6533453464508057),\n",
       " ('iranians', 0.6436753273010254),\n",
       " ('irandeal', 0.6365297436714172),\n",
       " ('hezbollah', 0.6304106116294861),\n",
       " ('jcpoa', 0.6227744817733765),\n",
       " ('destabilizing', 0.6011393070220947),\n",
       " ('russia', 0.5976725816726685),\n",
       " ('israel', 0.5941376090049744)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"iran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bush', 0.5577229261398315),\n",
       " ('incoming', 0.5056546926498413),\n",
       " ('elect', 0.4948301613330841),\n",
       " ('hillary', 0.49260416626930237),\n",
       " ('sanders', 0.4883331060409546),\n",
       " ('obamahe', 0.47077351808547974),\n",
       " ('macron', 0.4685414135456085),\n",
       " ('mkdbxpkquvshere', 0.46285107731819153),\n",
       " ('christie', 0.4538923501968384),\n",
       " ('cruz', 0.4513186514377594)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feeding US Presidents\n",
    "w2v_model.wv.most_similar([\"trump\",\"obama\", \"clinton\"])\n",
    "#First was Bush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Text -> Repsesenting each word by a number\n",
    "# Mapping of orginal word to number is preserved in word_index property of tokenizer\n",
    "\n",
    "#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10424, 29, 782, 3600, 15108, 37479, 3594, 1040, 621, 389]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check the first 10 words of first news\n",
    "#every word has been represented with a number\n",
    "X[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump -> 1\n",
      "said -> 2\n",
      "would -> 3\n",
      "president -> 4\n",
      "people -> 5\n",
      "one -> 6\n",
      "clinton -> 7\n",
      "state -> 8\n",
      "new -> 9\n",
      "obama -> 10\n"
     ]
    }
   ],
   "source": [
    "#Lets check few word to numerical replesentation\n",
    "#Mapping is preserved in dictionary -> word_index property of instance\n",
    "word_index = tokenizer.word_index\n",
    "for word, num in word_index.items():\n",
    "    print(f\"{word} -> {num}\")\n",
    "    if num == 10:\n",
    "        break     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For determining size of input...\n",
    "\n",
    "# Making histogram for no of words in news shows that most news article are under 700 words.\n",
    "# Lets keep each news small and truncate all news to 700 while tokenizing\n",
    "plt.hist([len(x) for x in X], bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nos = np.array([len(x) for x in X])\n",
    "len(nos[nos  < 700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Word2Vec' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-75ddb15b1e0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0membedding_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_weight_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-9b40a3ef2451>\u001b[0m in \u001b[0;36mget_weight_matrix\u001b[1;34m(model, vocab)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# step vocab, store vectors using the Tokenizer's integer mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mweight_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mweight_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2Vec' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
    "embedding_vectors = get_weight_matrix(w2v_model, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c0936e4d3f0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Non-trainable embeddidng layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_vectors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=100, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n",
    "#LSTM \n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "del embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.3, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction is in probability of news being real, so converting into classes\n",
    "# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)\n",
    "y_pred = (model.predict(X_test) >= 0.5).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
